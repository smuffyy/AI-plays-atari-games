# -*- coding: utf-8 -*-
"""ResumeTraining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/114wQmZ4SoW-sO4phNObqUTr7Wt-hkSzc
"""



!pip install gym[atari] autorom[accept-rom-license] tf-agents

import gym
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import animation
import random
import tensorflow as tf
from gym.wrappers import TimeLimit
from tf_agents.environments import suite_gym
from tf_agents.environments import suite_atari
from tf_agents.environments.atari_preprocessing import AtariPreprocessing
from tf_agents.environments.atari_wrappers import FrameStack4
from tf_agents.environments.tf_py_environment import TFPyEnvironment
from tf_agents.networks.q_network import QNetwork
from tf_agents.agents.dqn.dqn_agent import DqnAgent
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.eval.metric_utils import log_metrics
import logging
from tf_agents.metrics import tf_metrics
from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver
from tf_agents.policies.random_tf_policy import RandomTFPolicy
from tf_agents.utils.common import function
import matplotlib as mpl
import matplotlib.animation as animation
import PIL
import os

max_episode_steps=50000
environment_name = "SpaceInvaders-v4"  # Change to Space Invaders
env = suite_atari.load(
    environment_name,
    gym_env_wrappers=[AtariPreprocessing, FrameStack4])

# Construct the Q network using observation specifications
preprocessing_layer = tf.keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)
conv_layer_params = [(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]
fc_layer_params = [512]
q_net = QNetwork(
    env.observation_space.shape,
    env.action_space.n,
    preprocessing_layers=preprocessing_layer,
    conv_layer_params=conv_layer_params,
    fc_layer_params=fc_layer_params
)

# Set up the agent for training
optimizer = tf.keras.optimizers.RMSprop(
    learning_rate=2.5e-4,
    rho=0.95,
    momentum=0.0,
    epsilon=0.000001,
    centered=True
)
train_step = tf.Variable(0)
update_period = 4

agent = DqnAgent(
    env.time_limit,
    env.action_space,
    q_network=q_net,
    optimizer=optimizer,
    target_update_period=2000,
    td_errors_loss_fn=tf.keras.losses.Huber(reduction="none"),
    gamma=0.99,
    train_step_counter=train_step
)
agent.initialize()

# Set up replay buffer
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=env.batch_size,
    max_length=100000
)

# Train the agent
def train_agent(n_iterations):
    time_step = None
    policy_state = agent.collect_policy.get_initial_state(env.batch_size)
    iterator = iter(replay_buffer.as_dataset())
    for iteration in range(n_iterations):
        time_step, policy_state = collect_data(env, agent.collect_policy, time_step, policy_state)
        trajectories, _ = next(iterator)
        train_loss = agent.train(trajectories)
        print("\r{} loss:{:.5f}".format(iteration, train_loss.loss.numpy()), end="")
        if iteration % 1000 == 0:
            agent.save_checkpoint()  # Save the model checkpoint

# Resume training
train_agent(n_iterations=30000)