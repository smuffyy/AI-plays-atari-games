# -*- coding: utf-8 -*-
"""BuildAndTrainAgentPong.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/114wQmZ4SoW-sO4phNObqUTr7Wt-hkSzc
"""

!pip install gym[atari] autorom[accept-rom-license] tf-agents
# import the dependencies

import gym
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import animation
import random
import tensorflow as tf
from gym.wrappers import TimeLimit
from tf_agents.environments import suite_gym
from tf_agents.environments import suite_atari
from tf_agents.environments.atari_preprocessing import AtariPreprocessing
from tf_agents.environments.atari_wrappers import FrameStack4
from tf_agents.environments.tf_py_environment import TFPyEnvironment
from tf_agents.networks.q_network import QNetwork
from tf_agents.agents.dqn.dqn_agent import DqnAgent
from tf_agents.replay_buffers import tf_uniform_replay_buffer
from tf_agents.eval.metric_utils import log_metrics
import logging
from tf_agents.metrics import tf_metrics
from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver
from tf_agents.policies.random_tf_policy import RandomTFPolicy
from tf_agents.utils.common import function
import matplotlib as mpl
import matplotlib.animation as animation
import PIL
import os
import matplotlib

#Load the environment
max_episode_steps = 50000
environment_name = "Pong-v4"  # Change to Space Invaders
env = suite_atari.load(
    environment_name,
    max_episode_steps=max_episode_steps,
    gym_env_wrappers=[AtariPreprocessing, FrameStack4])

tf_env = TFPyEnvironment(env)

print(tf_env.action_spec())
print(tf_env.observation_spec())

#Test the environment

env.seed(42)
env.reset()
time_step = env.step(np.array(1)) # FIRE
for _ in range(4):
  time_step = env.step(np.array(3)) # LEFT

def plot_observation(obs):
  obs = obs.astype(np.float32)
  img = obs[..., :3]
  current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)
  img[..., 0] += current_frame_delta
  img[..., 2] += current_frame_delta
  img = np.clip(img / 150, 0, 1)
  plt.imshow(img)
  plt.axis("off")
  plt.figure(figsize=(6, 6))
plot_observation(time_step.observation)
plt.show()

#Construct the Qnetwork using observation specifications
preprocessing_layer = tf.keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)
conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]
fc_layer_params=[512]

q_net = QNetwork(
    tf_env.observation_spec(),
    tf_env.action_spec(),
    preprocessing_layers=preprocessing_layer,
    conv_layer_params=conv_layer_params,
    fc_layer_params=fc_layer_params)

#set up the agent for training using the DQN algorithm with the specified configuration.
optimizer = tf.keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0,
                                        epsilon=0.000001, centered=True)
train_step = tf.Variable(0)
update_period = 4 # run a training step every 4 collect steps

epsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate=1.0, # initial ?
    decay_steps=250000000 // update_period, # <=> 1,000,000 ALE frames
    end_learning_rate=0.01) # final ?

agent = DqnAgent(tf_env.time_step_spec(),
                 tf_env.action_spec(),
                 q_network=q_net,
                 optimizer=optimizer,
                 target_update_period=2000, # <=> 32,000 ALE frames
                 td_errors_loss_fn=tf.keras.losses.Huber(reduction="none"),
                 gamma=0.99, # discount factor
                 train_step_counter=train_step,
                 epsilon_greedy=lambda: epsilon_fn(train_step))

agent.initialize()

#set up replay bufffer and train metrics
replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=tf_env.batch_size,
    max_length=100000)
replay_buffer_observer = replay_buffer.add_batch

train_metrics = [
    tf_metrics.NumberOfEpisodes(),
    tf_metrics.EnvironmentSteps(),
    tf_metrics.AverageReturnMetric(),
    tf_metrics.AverageEpisodeLengthMetric(),
]
logging.getLogger().setLevel(logging.INFO)
log_metrics(train_metrics)

collect_driver = DynamicStepDriver(tf_env,
                                   agent.collect_policy,
                                   observers=[replay_buffer_observer] + train_metrics,
                                   num_steps=update_period)
#define the progressupdate class
class ProgressUpdate:
    def __init__(self, total):
        self.counter = 0
        self.total = total

    def __call__(self, trajectory):
        if not trajectory.is_boundary():
            self.counter += 1
        if self.counter % 100 == 0:
            print(f"\r{self.counter}/{self.total}", end="")


initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),
                                        tf_env.action_spec())
init_driver = DynamicStepDriver(tf_env,
                                initial_collect_policy,
                                observers=[replay_buffer.add_batch, ProgressUpdate(20000)],
                                num_steps=20000)
final_time_step, final_policy_state = init_driver.run()

dataset = replay_buffer.as_dataset(sample_batch_size=64,
                                   num_steps=2,
                                   num_parallel_calls=3).prefetch(3)

collect_driver.run = function(collect_driver.run)
agent.train = function(agent.train)

#train the agent
def train_agent(n_iterations):
  time_step = None
  policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)
  iterator = iter(dataset)
  for iteration in range(n_iterations):
    time_step, policy_state = collect_driver.run(time_step, policy_state)
    trajectories, buffer_info = next(iterator)
    train_loss = agent.train(trajectories)
    print("\r{} loss:{:.5f}".format(
    iteration, train_loss.loss.numpy()), end="")
    if iteration % 1000 == 0:
      log_metrics(train_metrics)

train_agent(n_iterations=30000)

#create frames and save to collect for video
frames = []
def save_frames(trajectory):
  global frames
  frames.append(tf_env.pyenv.envs[0].render(mode="rgb_array"))

watch_driver = DynamicStepDriver(tf_env,
                                 agent.policy,
                                 observers=[save_frames, ProgressUpdate(30000)],
                                 num_steps=30000)

final_time_step, final_policy_state = watch_driver.run()

matplotlib.rcParams['animation.embed_limit'] = 2**128

mpl.rc('animation', html='jshtml')

def update_scene(num, frames, patch):
  patch.set_data(frames[num])
  return patch,

import time

def plot_animation(frames, repeat=False, interval=40):
    fig = plt.figure()
    patch = plt.imshow(frames[0])
    plt.axis('off')
    anim = animation.FuncAnimation(fig, update_scene, fargs=(frames, patch),
    frames=len(frames), repeat=repeat, interval=interval)
    plt.close()
    plt.show()


plot_animation(frames)

#save the resluting gif
image_path = os.path.join("./Pong.gif")
frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]
frame_images[0].save(image_path, format='GIF',
                     append_images=frame_images[1:],
                     save_all=True,
                     duration=30,
                     loop=0)